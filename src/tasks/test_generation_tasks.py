"""Task definitions for the API test generation workflow."""

from crewai import Task, Agent


def create_parse_task(
    agent: Agent,
    scenario_text: str
) -> Task:
    """Create a task for parsing API test scenarios.
    
    Args:
        agent: The Parser Agent
        scenario_text: Raw scenario text to parse
        
    Returns:
        Configured Task
    """
    return Task(
        description=f"""
Parse the following API test scenario and extract structured information.

## Scenario
{scenario_text}

## Your Task
1. Identify the input format (Gherkin, plain English, or user story)
2. Extract the feature name and scenario name
3. Identify the API endpoint URL and HTTP method
4. Extract authentication details (token URL, credentials)
5. Break down into individual test steps
6. For each step, identify:
   - Step type (Given/When/Then/And)
   - Description of the step
   - Action to perform (get_token, send_request, verify_status, etc.)
   - HTTP method, endpoint, payload details
   - Expected values for assertions

## Output
Provide a structured JSON response with:
- feature_name: Name of the API feature being tested
- scenario_name: Name of the specific scenario
- api_endpoint: Full URL of the API endpoint
- http_method: GET, POST, PUT, DELETE, etc.
- authentication: Token URL, credentials, grant type
- steps: Array of step objects
""",
        expected_output="""A JSON object containing:
{
  "feature_name": "string",
  "scenario_name": "string",
  "api_endpoint": "string",
  "http_method": "POST|GET|PUT|DELETE",
  "authentication": {
    "token_url": "string",
    "username": "string",
    "password": "string",
    "grant_type": "client_credentials"
  },
  "steps": [
    {
      "step_type": "given|when|then|and",
      "description": "step description",
      "action": "get_token|send_request|verify_status|verify_response",
      "details": {}
    }
  ]
}""",
        agent=agent,
    )


def create_analyze_task(
    agent: Agent,
    context: list[Task]
) -> Task:
    """Create a task for analyzing parsed API scenarios.
    
    Args:
        agent: The Analyzer Agent
        context: Previous tasks to use as context
        
    Returns:
        Configured Task
    """
    return Task(
        description="""
Analyze the parsed API test scenario and create a detailed test blueprint.

## Your Task
Using the parsed scenario from the previous step:

1. Identify authentication flow (OAuth2, API key, etc.)
2. Map test steps to requests library actions
3. Determine expected status codes and response validations
4. Identify test data requirements and payload structure

## API Action Mapping
- "get_token" → session.post() to token endpoint with credentials
- "send_request" → session.post/get/put/delete() to API endpoint
- "verify_status" → assert response.status_code == expected
- "verify_response" → assert on response.json() fields

## Output
Provide a test blueprint with:
- class_name: PascalCase test class name
- test_methods: List of test methods to generate
- authentication: OAuth2 token configuration
- base_payload: Template payload for the API
- assertions: Expected status codes and response validations
""",
        expected_output="""A JSON test blueprint containing:
{
  "class_name": "TestAPIFeatureName",
  "test_methods": [
    {
      "method_name": "test_scenario_name",
      "test_type": "positive|negative",
      "docstring": "Verify that...",
      "payload_modifications": {},
      "expected_status": 200|201|400|404,
      "assertions": []
    }
  ],
  "authentication": {
    "token_url": "...",
    "username": "...",
    "password": "...",
    "grant_type": "client_credentials"
  },
  "base_payload": {}
}""",
        agent=agent,
        context=context,
    )


def create_generate_task(
    agent: Agent,
    context: list[Task]
) -> Task:
    """Create a task for generating pytest API test code.
    
    Args:
        agent: The Generator Agent
        context: Previous tasks to use as context
        
    Returns:
        Configured Task
    """
    return Task(
        description="""
Generate a complete pytest API test file from the test blueprint.

## Your Task
Using the test blueprint from the previous step, generate:

1. A complete, runnable Python test file using pytest and requests
2. All necessary imports (pytest, requests, json, urllib3)
3. A test class with proper OAuth2 authentication fixture
4. Test method(s) for each scenario with docstrings
5. Given/When/Then comments to organize the code
6. Proper assertions on status codes and response data

## Code Structure - USE THIS EXACT TEMPLATE:
```python
\"\"\"API Test module for [Feature Name].

Auto-generated by AI Test Case Generator.
API Endpoint: [METHOD] [URL]
\"\"\"

import pytest
import requests
import json
import urllib3

# Suppress SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class TestClassName:
    \"\"\"API Test cases for [Feature Name].\"\"\"
    
    # Token API Configuration
    TOKEN_URL = "[token_url]"
    TOKEN_USERNAME = "[username]"
    TOKEN_PASSWORD = "[password]"
    
    # API Configuration
    API_BASE_URL = "[base_url]"
    ENDPOINT = "[endpoint]"
    
    @pytest.fixture(autouse=True)
    def setup(self):
        \"\"\"Setup for each test - obtain OAuth2 token.\"\"\"
        self.session = requests.Session()
        self.session.verify = False
        
        # Get OAuth2 token
        self.access_token = self._get_access_token()
        
        # Set headers with Bearer token
        self.headers = {
            "Content-Type": "application/json",
            "Accept": "application/json",
            "Authorization": f"Bearer {{self.access_token}}"
        }
        
        yield
        self.session.close()
    
    def _get_access_token(self) -> str:
        \"\"\"Obtain OAuth2 access token.\"\"\"
        token_payload = {
            "grant_type": "client_credentials",
            "scope": "full"
        }
        
        response = self.session.post(
            self.TOKEN_URL,
            data=token_payload,
            auth=(self.TOKEN_USERNAME, self.TOKEN_PASSWORD),
            headers={{"Content-Type": "application/x-www-form-urlencoded"}},
            verify=False
        )
        
        if response.status_code != 200:
            pytest.fail(f"Failed to get token: {{response.status_code}} - {{response.text}}")
        
        return response.json().get("access_token")
    
    def get_base_payload(self) -> dict:
        \"\"\"Return the base payload for the API.\"\"\"
        return {{
            # Payload from scenario
        }}
    
    @pytest.mark.positive
    def test_positive_scenario(self):
        \"\"\"Verify [positive scenario description].\"\"\"
        payload = self.get_base_payload()
        
        response = self.session.post(
            f"{{self.API_BASE_URL}}{{self.ENDPOINT}}",
            json=payload,
            headers=self.headers,
            verify=False
        )
        
        assert response.status_code in [200, 201], f"Expected 200/201, got {{response.status_code}}"
    
    @pytest.mark.negative
    def test_negative_scenario(self):
        \"\"\"Verify [negative scenario description].\"\"\"
        payload = self.get_base_payload()
        payload["field"] = invalid_value  # Modify for negative test
        
        response = self.session.post(
            f"{{self.API_BASE_URL}}{{self.ENDPOINT}}",
            json=payload,
            headers=self.headers,
            verify=False
        )
        
        assert response.status_code == 400, f"Expected 400, got {{response.status_code}}"
```

## CRITICAL Requirements
- Use requests library, NOT playwright
- Use pytest fixtures for setup
- Disable SSL verification (verify=False)
- Suppress urllib3 warnings
- Include debug print statements
- Use assert statements (not expect)
- Extract constants for URLs and credentials

## Output
Provide ONLY the complete Python code. No explanations or markdown wrappers.
""",
        expected_output="""Complete Python API test code that:
- Has all necessary imports (pytest, requests, json, urllib3)
- Contains OAuth2 authentication fixture
- Has test methods with docstrings
- Uses requests library for API calls
- Is properly formatted and ready to run with pytest""",
        agent=agent,
        context=context,
    )


def create_review_task(
    agent: Agent,
    context: list[Task]
) -> Task:
    """Create a task for reviewing generated API test code.
    
    Args:
        agent: The Reviewer Agent
        context: Previous tasks to use as context
        
    Returns:
        Configured Task
    """
    return Task(
        description="""
Review the generated API test code for quality and correctness.

## Your Task
1. Check that all imports are present and correct
2. Verify the code is syntactically correct Python
3. Ensure API testing best practices are followed
4. Check that assertions are proper
5. Verify OAuth2 authentication is implemented correctly
6. Check for proper documentation

## Review Checklist
- [ ] `import pytest` present
- [ ] `import requests` present
- [ ] `import json` present
- [ ] `import urllib3` with warning suppression
- [ ] Class name starts with "Test"
- [ ] Method names start with "test_"
- [ ] OAuth2 token retrieval implemented
- [ ] `verify=False` for SSL handling
- [ ] `assert` statements used (NOT expect)
- [ ] Docstrings are present and descriptive
- [ ] Code follows PEP 8
- [ ] Debug print statements included

## If Issues Found
- Fix any syntax errors
- Add missing imports
- Correct authentication flow
- Add missing SSL handling
- Improve documentation

## Output
Return ONLY the final corrected Python code. No JSON wrapper, no markdown, just the code.
""",
        expected_output="""Complete, reviewed Python API test code ready to run with pytest.
The code should be the final version with all corrections applied.""",
        agent=agent,
        context=context,
    )


def create_github_task(
    agent: Agent,
    files: list[str],
    feature_name: str,
    base_branch: str = "main"
) -> Task:
    """Create a task for pushing to GitHub and creating a PR.
    
    Args:
        agent: The GitHub Agent
        files: List of generated file paths
        feature_name: Name of the feature for the PR
        base_branch: Target branch for the PR
        
    Returns:
        Configured Task
    """
    files_str = ", ".join(files)
    
    return Task(
        description=f"""
Push the generated API test files to GitHub and create a Pull Request.

## Files to Commit
{files_str}

## Feature Name
{feature_name}

## Target Branch
{base_branch}

## Your Task
1. Check the current Git status
2. Create a new branch with a descriptive name
3. Stage and commit the generated test files
4. Push the branch to origin
5. Create a Pull Request

## Branch Naming
Use format: test/auto-generated-[feature]-[timestamp]

## Commit Message
Use format: "Add auto-generated API tests for [feature]"

## PR Details
- Title: "[Auto-Generated] Add API tests for [feature]"
- Body: Include summary of tests, file list, and run instructions

## Output
Provide a summary of actions taken and the PR URL (if successful).
""",
        expected_output="""Summary of GitHub operations:
- Branch created: [branch name]
- Files committed: [count]
- Push status: success/failure
- PR URL: [url] or error message""",
        agent=agent,
    )
